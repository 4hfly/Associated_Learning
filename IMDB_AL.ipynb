{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.6.7 64-bit"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.7","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"interpreter":{"hash":"30295c5bec572e859485b1ffa5e89b8b3e2022ef6e3e739c1ac40f143a557caf"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Sentiment Analysis using LSTM"],"metadata":{}},{"cell_type":"markdown","source":["## What is Sentiment Analysis:\n","the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.\n","\n","I think this result from google dictionary gives a very succinct definition. I don’t have to re-emphasize how important sentiment analysis has become. So, here we will build a classifier on IMDB movie dataset using a Deep Learning technique called RNN.\n","\n","I’m outlining a step-by-step process for how Recurrent Neural Networks (RNN) can be implemented using Long Short Term Memory (LSTM) architecture:\n","\n","1. Load in and visualize the data\n","2. Data Processing — convert to lower case, Remove punctuation etc.\n","5. Tokenize — Create Vocab to Int mapping dictionary\n","6. Tokenize — Encode the words\n","7. Tokenize — Encode the labels\n","8. Analyze Reviews Length\n","9. Removing Outliers — Getting rid of extremely long or short reviews\n","10. Padding / Truncating the remaining data\n","11. Training, Validation, Test Dataset Split\n","12. Dataloaders and Batching\n","13. Define the LSTM Network Architecture\n","14. Define the Model Class\n","15. Training the Network\n","16. Testing"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import re\n","import string\n","from collections import Counter\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","\n","# from classification.model import EmbeddingAL, LSTMAL"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:22:07.481785Z","iopub.execute_input":"2021-08-14T14:22:07.48219Z","iopub.status.idle":"2021-08-14T14:22:09.971369Z","shell.execute_reply.started":"2021-08-14T14:22:07.482075Z","shell.execute_reply":"2021-08-14T14:22:09.970547Z"},"trusted":true}},{"cell_type":"code","execution_count":2,"source":["from classification.model import EmbeddingAL, LSTMAL"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["def get_n_params(model):\n","    pp = 0\n","    for p in list(model.parameters()):\n","        nn = 1\n","        for s in list(p.size()):\n","            nn = nn*s\n","        pp += nn\n","    return pp"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## 1) Load in and visualize the data"],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["df = pd.read_csv('IMDB_Dataset.csv')\n","df.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":4}],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:22:19.611868Z","iopub.execute_input":"2021-08-14T14:22:19.612203Z","iopub.status.idle":"2021-08-14T14:22:21.002699Z","shell.execute_reply.started":"2021-08-14T14:22:19.612169Z","shell.execute_reply":"2021-08-14T14:22:21.001939Z"},"trusted":true}},{"cell_type":"markdown","source":["## 2) Data Processing — convert to lower case, Remove punctuation etc"],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["def data_preprocessing(text):\n","    text = text.lower()\n","    text = re.sub('<.*?>', '', text) # Remove HTML from text\n","    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation\n","    text = [word for word in text.split() if word not in stop_words]\n","    text = ' '.join(text)\n","    return text"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:22:24.055754Z","iopub.execute_input":"2021-08-14T14:22:24.056078Z","iopub.status.idle":"2021-08-14T14:22:33.321151Z","shell.execute_reply.started":"2021-08-14T14:22:24.056048Z","shell.execute_reply":"2021-08-14T14:22:33.320349Z"},"trusted":true}},{"cell_type":"markdown","source":["## 5) Tokenize — Create Vocab to Int mapping dictionary\n","In most of the NLP tasks, you will create an index mapping dictionary in such a way that your frequently occurring words are assigned lower indexes. One of the most common way of doing this is to use Counter method from Collections library."],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["df['cleaned_reviews'] = df['review'].apply(data_preprocessing)\n","corpus = [word for text in df['cleaned_reviews'] for word in text.split()]\n","count_words = Counter(corpus)\n","sorted_words = count_words.most_common()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:22:38.410797Z","iopub.execute_input":"2021-08-14T14:22:38.411137Z","iopub.status.idle":"2021-08-14T14:22:40.190972Z","shell.execute_reply.started":"2021-08-14T14:22:38.411084Z","shell.execute_reply":"2021-08-14T14:22:40.190177Z"},"trusted":true}},{"cell_type":"markdown","source":["There is a small trick here, in this mapping index will start from 0 i.e. mapping of ‘the’ will be 0. But later on we are going to do padding for shorter reviews and conventional choice for padding is 0. So we need to start this indexing from 1"],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n","\n","reviews_int = []\n","for text in df['cleaned_reviews']:\n","    r = [vocab_to_int[word] for word in text.split()]\n","    reviews_int.append(r)\n","\n","# print(reviews_int[:1])\n","df['Review int'] = reviews_int"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:22:53.253787Z","iopub.execute_input":"2021-08-14T14:22:53.254109Z","iopub.status.idle":"2021-08-14T14:22:55.395335Z","shell.execute_reply.started":"2021-08-14T14:22:53.254079Z","shell.execute_reply":"2021-08-14T14:22:55.39454Z"},"trusted":true}},{"cell_type":"markdown","source":["## 7) Tokenize — Encode the labels\n","This is simple because we only have 2 output labels. So, we will just label ‘positive’ as 1 and ‘negative’ as 0"],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n","# df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:23:07.119929Z","iopub.execute_input":"2021-08-14T14:23:07.120323Z","iopub.status.idle":"2021-08-14T14:23:07.17025Z","shell.execute_reply.started":"2021-08-14T14:23:07.120291Z","shell.execute_reply":"2021-08-14T14:23:07.169246Z"},"trusted":true}},{"cell_type":"markdown","source":["## 8) Analyze Reviews Length"],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["review_len = [len(x) for x in reviews_int]\n","df['Review len'] = review_len\n","# df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:23:10.45787Z","iopub.execute_input":"2021-08-14T14:23:10.458204Z","iopub.status.idle":"2021-08-14T14:23:10.499486Z","shell.execute_reply.started":"2021-08-14T14:23:10.458173Z","shell.execute_reply":"2021-08-14T14:23:10.498592Z"},"trusted":true}},{"cell_type":"code","execution_count":10,"source":["print(df['Review len'].describe())\n","\n","df['Review len'].hist()\n","# plt.title('Review length distribution', size=15)\n","# plt.show()"],"outputs":[{"output_type":"stream","name":"stdout","text":["count    50000.000000\n","mean       119.855700\n","std         90.096619\n","min          3.000000\n","25%         64.000000\n","50%         89.000000\n","75%        146.000000\n","max       1429.000000\n","Name: Review len, dtype: float64\n"]},{"output_type":"execute_result","data":{"text/plain":["<AxesSubplot:>"]},"metadata":{},"execution_count":10},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 388.0125 248.518125\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-15T20:20:27.319299</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 388.0125 248.518125 \nL 388.0125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 46.0125 224.64 \nL 380.8125 224.64 \nL 380.8125 7.2 \nL 46.0125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 61.230682 224.64 \nL 91.667045 224.64 \nL 91.667045 17.554286 \nL 61.230682 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 91.667045 224.64 \nL 122.103409 224.64 \nL 122.103409 172.058822 \nL 91.667045 172.058822 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 122.103409 224.64 \nL 152.539773 224.64 \nL 152.539773 212.352818 \nL 122.103409 212.352818 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 152.539773 224.64 \nL 182.976136 224.64 \nL 182.976136 220.461363 \nL 152.539773 220.461363 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 182.976136 224.64 \nL 213.4125 224.64 \nL 213.4125 224.479708 \nL 182.976136 224.479708 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 213.4125 224.64 \nL 243.848864 224.64 \nL 243.848864 224.612364 \nL 213.4125 224.612364 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 243.848864 224.64 \nL 274.285227 224.64 \nL 274.285227 224.617891 \nL 243.848864 224.617891 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 274.285227 224.64 \nL 304.721591 224.64 \nL 304.721591 224.628945 \nL 274.285227 224.628945 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 304.721591 224.64 \nL 335.157955 224.64 \nL 335.157955 224.634473 \nL 304.721591 224.634473 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#pb78d15273a)\" d=\"M 335.157955 224.64 \nL 365.594318 224.64 \nL 365.594318 224.634473 \nL 335.157955 224.634473 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 60.590366 224.64 \nL 60.590366 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"meb80ee2448\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.590366\" xlink:href=\"#meb80ee2448\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(57.409116 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 103.278113 224.64 \nL 103.278113 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.278113\" xlink:href=\"#meb80ee2448\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(93.734363 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 145.96586 224.64 \nL 145.96586 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"145.96586\" xlink:href=\"#meb80ee2448\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(136.42211 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 188.653607 224.64 \nL 188.653607 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.653607\" xlink:href=\"#meb80ee2448\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(179.109857 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 231.341354 224.64 \nL 231.341354 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"231.341354\" xlink:href=\"#meb80ee2448\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <g transform=\"translate(221.797604 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 274.029101 224.64 \nL 274.029101 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"274.029101\" xlink:href=\"#meb80ee2448\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1000 -->\n      <g transform=\"translate(261.304101 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 316.716848 224.64 \nL 316.716848 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"316.716848\" xlink:href=\"#meb80ee2448\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1200 -->\n      <g transform=\"translate(303.991848 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 359.404595 224.64 \nL 359.404595 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"359.404595\" xlink:href=\"#meb80ee2448\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1400 -->\n      <g transform=\"translate(346.679595 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 46.0125 224.64 \nL 380.8125 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1063cd6f7b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m1063cd6f7b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(32.65 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 46.0125 197.003514 \nL 380.8125 197.003514 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m1063cd6f7b\" y=\"197.003514\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5000 -->\n      <g transform=\"translate(13.5625 200.802733)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 46.0125 169.367029 \nL 380.8125 169.367029 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m1063cd6f7b\" y=\"169.367029\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10000 -->\n      <g transform=\"translate(7.2 173.166247)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 46.0125 141.730543 \nL 380.8125 141.730543 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m1063cd6f7b\" y=\"141.730543\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 15000 -->\n      <g transform=\"translate(7.2 145.529762)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 46.0125 114.094057 \nL 380.8125 114.094057 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m1063cd6f7b\" y=\"114.094057\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 20000 -->\n      <g transform=\"translate(7.2 117.893276)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 46.0125 86.457572 \nL 380.8125 86.457572 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m1063cd6f7b\" y=\"86.457572\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 25000 -->\n      <g transform=\"translate(7.2 90.25679)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 46.0125 58.821086 \nL 380.8125 58.821086 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m1063cd6f7b\" y=\"58.821086\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 30000 -->\n      <g transform=\"translate(7.2 62.620305)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#pb78d15273a)\" d=\"M 46.0125 31.1846 \nL 380.8125 31.1846 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m1063cd6f7b\" y=\"31.1846\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 35000 -->\n      <g transform=\"translate(7.2 34.983819)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 46.0125 224.64 \nL 46.0125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 380.8125 224.64 \nL 380.8125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 46.0125 224.64 \nL 380.8125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 46.0125 7.2 \nL 380.8125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb78d15273a\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.0125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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"},"metadata":{"needs_background":"light"}}],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:23:33.090255Z","iopub.execute_input":"2021-08-14T14:23:33.09059Z","iopub.status.idle":"2021-08-14T14:23:33.267935Z","shell.execute_reply.started":"2021-08-14T14:23:33.09056Z","shell.execute_reply":"2021-08-14T14:23:33.267014Z"},"trusted":true}},{"cell_type":"markdown","source":["<b>Observations : </b>a) Mean review length = 226 b) Most of the reviews less than 500 words or more d) There are quite a few reviews that are extremely long, we can manually investigate them to check whether we need to include or exclude them from our analysis"],"metadata":{}},{"cell_type":"markdown","source":["## 10) Padding / Truncating the remaining data\n","To deal with both short and long reviews, we will pad or truncate all our reviews to a specific length. We define this length by Sequence Length. This sequence length is same as number of time steps for LSTM layer.\n","\n","For reviews shorter than seq_length, we will pad with 0s. For reviews longer than seq_length we will truncate them to the first seq_length words."],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["def Padding(review_int, seq_len):\n","    '''\n","    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n","    '''\n","    features = np.zeros((len(reviews_int), seq_len), dtype = int)\n","    for i, review in enumerate(review_int):\n","        if len(review) <= seq_len:\n","            zeros = list(np.zeros(seq_len - len(review)))\n","            new = zeros + review\n","        else:\n","            new = review[: seq_len]\n","        features[i, :] = np.array(new)\n","            \n","    return features"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:23:42.837574Z","iopub.execute_input":"2021-08-14T14:23:42.837888Z","iopub.status.idle":"2021-08-14T14:23:42.84612Z","shell.execute_reply.started":"2021-08-14T14:23:42.837859Z","shell.execute_reply":"2021-08-14T14:23:42.845349Z"},"trusted":true}},{"cell_type":"code","execution_count":12,"source":["features = Padding(reviews_int, 200)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:23:46.50403Z","iopub.execute_input":"2021-08-14T14:23:46.504368Z","iopub.status.idle":"2021-08-14T14:23:49.323177Z","shell.execute_reply.started":"2021-08-14T14:23:46.504339Z","shell.execute_reply":"2021-08-14T14:23:49.321675Z"},"trusted":true}},{"cell_type":"markdown","source":["## 11) Training, Validation, Test Dataset Split\n","Once we have got our data in nice shape, we will split it into training, validation and test sets\n","\n","<b>train= 80% | valid = 10% | test = 10% </b>"],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["X_train, X_remain, y_train, y_remain = train_test_split(features, df['sentiment'].to_numpy(), test_size=0.2, random_state=1)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:23:53.080227Z","iopub.execute_input":"2021-08-14T14:23:53.08057Z","iopub.status.idle":"2021-08-14T14:23:53.141854Z","shell.execute_reply.started":"2021-08-14T14:23:53.080526Z","shell.execute_reply":"2021-08-14T14:23:53.140826Z"},"trusted":true}},{"cell_type":"markdown","source":["## 12) Dataloaders and Batching\n","After creating our training, test and validation data. Next step is to create dataloaders for this data. We can use generator function for batching our data into batches instead we will use a TensorDataset. This is one of a very useful utility in PyTorch for using our data with DataLoaders with exact same ease as of torchvision datasets"],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["# create tensor dataset\n","train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n","\n","# dataloaders\n","batch_size = 16\n","\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:23:58.569377Z","iopub.execute_input":"2021-08-14T14:23:58.569691Z","iopub.status.idle":"2021-08-14T14:23:58.585035Z","shell.execute_reply.started":"2021-08-14T14:23:58.569662Z","shell.execute_reply":"2021-08-14T14:23:58.584042Z"},"trusted":true}},{"cell_type":"code","execution_count":15,"source":["# obtain one batch of training data\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:24:03.072622Z","iopub.execute_input":"2021-08-14T14:24:03.072946Z","iopub.status.idle":"2021-08-14T14:24:03.117205Z","shell.execute_reply.started":"2021-08-14T14:24:03.072916Z","shell.execute_reply":"2021-08-14T14:24:03.116321Z"},"trusted":true}},{"cell_type":"markdown","source":["## 13) Define the LSTM Network Architecture\n","\n","<img src='https://miro.medium.com/max/700/1*SICYykT7ybua1gVJDNlajw.png'>\n","\n","The layers are as follows:\n","\n","0. Tokenize : This is not a layer for LSTM network but a mandatory step of converting our words into tokens (integers)\n","1. Embedding Layer: that converts our word tokens (integers) into embedding of specific size\n","2. LSTM Layer: defined by hidden state dims and number of layers\n","3. Fully Connected Layer: that maps output of LSTM layer to a desired output size\n","4. Sigmoid Activation Layer: that turns all output values in a value between 0 and 1\n","5. Output: Sigmoid output from the last timestep is considered as the final output of this network\n"],"metadata":{}},{"cell_type":"markdown","source":["## 14) Define the Model Class"],"metadata":{}},{"cell_type":"code","execution_count":24,"source":["class SentAL(nn.Module):\n","    def __init__(self, emb, l1, l2):\n","        super(SentAL, self).__init__()\n","        self.embedding = emb\n","        self.layer_1 = l1\n","        self.layer_2 = l2\n","    def forward(self, x, y):\n","        emb_x, emb_y, = self.embedding(x, y)\n","        print(self.embedding._t_prime.shape, self.embedding.y.shape)\n","        emb_loss = self.embedding.loss()\n","        \n","        layer_1_x, h1, layer_1_y = self.layer_1(emb_x, emb_y)\n","        layer_1_loss = self.layer_1.loss()\n","\n","        h,c = h1\n","        print('old', h.shape, c.shape)\n","        h = h.reshape(h.size(0), h.size(1), 2, -1)\n","        h = h[:,:,0,:] + h[:,:,1,:]\n","        c = c.reshape(c.size(0), c.size(1), 2, -1)\n","        c = c[:,:,0,:] + c[:,:,1,:]\n","        print('new', h.shape, c.shape)\n","        h1 = (h,c)\n","\n","        layer_2_x, h2, layer_2_y = self.layer_2(layer_1_x, layer_1_y, h1)\n","        layer_2_loss = self.layer_2.loss()\n","\n","        return emb_loss, layer_1_loss, layer_2_loss\n"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:26:46.130833Z","iopub.execute_input":"2021-08-14T14:26:46.13117Z","iopub.status.idle":"2021-08-14T14:26:46.141747Z","shell.execute_reply.started":"2021-08-14T14:26:46.131121Z","shell.execute_reply":"2021-08-14T14:26:46.140689Z"},"trusted":true}},{"cell_type":"code","execution_count":21,"source":["torch.cuda.empty_cache()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":25,"source":["is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda:1\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")\n","\n","# Instantiate the model w/ hyperparams\n","vocab_size = len(vocab_to_int) + 1\n","output_size = 1\n","embedding_dim = 300\n","hidden_dim = 400\n","n_layers = 2\n","\n","emb = EmbeddingAL((vocab_size, 2), (300, 128))\n","l1 = LSTMAL(300,128, (128,128), dropout=0.2, bidirectional=True)\n","l2 = LSTMAL(128, 128, (64, 64), dropout=0.2, bidirectional=True)\n","\n","model = SentAL(emb, l1, l2)\n","model = model.to(device)\n","print(get_n_params(model))"],"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available\n","135339663\n"]},{"output_type":"stream","name":"stderr","text":["/home/hibb1126/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:26:48.37732Z","iopub.execute_input":"2021-08-14T14:26:48.377634Z","iopub.status.idle":"2021-08-14T14:26:49.211815Z","shell.execute_reply.started":"2021-08-14T14:26:48.377605Z","shell.execute_reply":"2021-08-14T14:26:49.210927Z"},"trusted":true}},{"cell_type":"markdown","source":["## Training Loop"],"metadata":{}},{"cell_type":"code","execution_count":27,"source":["lr=0.001\n","\n","criterion = nn.BCELoss()\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","optimizer_1 = torch.optim.Adam(model.embedding.parameters(), lr=1e-4)\n","optimizer_2 = torch.optim.Adam(model.layer_1.parameters(), lr=1e-4)\n","optimizer_3 = torch.optim.Adam(model.layer_2.parameters(), lr=1e-4)\n","# function to predict accuracy\n","def acc(pred,label):\n","    pred = torch.round(pred.squeeze())\n","    return torch.sum(pred == label.squeeze()).item()\n","\n","clip = 5\n","epochs = 10\n","valid_acc_min = np.Inf\n","# train for some number of epochs\n","epoch_tr_loss,epoch_vl_loss = [],[]\n","epoch_tr_acc,epoch_vl_acc = [],[]\n","\n","for epoch in range(epochs):\n","    train_losses = []\n","\n","    total_emb_loss = []\n","    total_l1_loss = []\n","    total_l2_loss = []\n","\n","    total_acc = 0.0\n","    total_count = 0\n","    model.embedding.train()\n","    model.layer_1.train()\n","    model.layer_2.train()\n","\n","    # initialize hidden state \n","    for inputs, labels in train_loader:\n","        \n","        print(labels.shape)\n","\n","        inputs, labels = inputs.to(device), labels.to(device)   \n","        \n","        model.zero_grad()\n","        emb_loss, l1_loss, l2_loss = model(inputs,labels)\n","        \n","        nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        total_emb_loss.append(emb_loss.item())\n","        total_l1_loss.append(l1_loss.item())\n","        total_l2_loss.append(l2_loss.item())\n","\n","        emb_loss.backward()\n","        optimizer_1.step()\n","\n","        l1_loss.backward()\n","        optimizer_2.step()\n","\n","        l2_loss.backward()\n","        optimizer_3.step()\n","\n","        torch.cuda.empty_cache()\n","\n","        # calculate the loss and perform backprop\n","        with torch.no_grad():\n","            model.embedding.eval()\n","            model.layer_1.eval()\n","            model.layer_2.eval()\n","\n","            left = model.layer_2.f(model.layer_1.f(model.embedding.f(inputs)))\n","            right = model.layer_2.bx(left)\n","            predicted_label = model.embedding.dy(\n","                model.layer_1.dy(model.layer_2.dy(right)))\n","\n","            total_acc += (predicted_label.argmax(1) == labels).sum().item()\n","            total_count += labels.size(0)\n","\n","        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(model.parameters(), clip)\n"," \n","    \n","    val_losses = []\n","    val_acc = 0.0\n","    val_count = 0\n","\n","    model.embedding.eval()\n","    model.layer_1.eval()\n","    model.layer_2.eval()\n","\n","    for inputs, labels in valid_loader:\n","        with torch.no_grad():\n","            model.embedding.eval()\n","            model.layer_1.eval()\n","            model.layer_2.eval()\n","\n","            left = model.layer_2.f(model.layer_1.f(model.embedding.f(inputs)))\n","            right = model.layer_2.bx(left)\n","            predicted_label = model.embedding.dy(\n","                model.layer_1.dy(model.layer_2.dy(right)))\n","\n","            val_acc += (predicted_label.argmax(1) == labels).sum().item()\n","            val_count += labels.size(0)\n","            \n","    epoch_train_loss = [np.mean(total_emb_loss), np.mean(total_l1_loss), np.mean(total_l2_loss)]\n","    epoch_train_acc = total_acc/total_count\n","    epoch_val_acc = val_acc/val_count\n","\n","    print(f'Epoch {epoch+1}') \n","    print(f'train_loss : {epoch_train_loss}')\n","    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n","    if epoch_val_acc <= valid_acc_min:\n","        torch.save(model.state_dict(), 'al.state_dict.pt')\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_acc_min,epoch_val_acc))\n","        valid_acc_min = epoch_val_acc\n","    print(25*'==')"],"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16])\n","torch.Size([16, 1]) torch.Size([16])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"input.size(-1) must be equal to input_size. Expected 128, got 256","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-bc8bbc15e1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0memb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-958bbfa9a972>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mlayer_2_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_2_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_1_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_1_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mlayer_2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/AL/Associated_Learning/classification/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, hx, hy)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_h_nx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_nx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_s_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    618\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                            ):\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    622\u001b[0m                                'Expected hidden[0] size {}, got {}')\n","\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise RuntimeError(\n\u001b[1;32m    206\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 207\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 128, got 256"]}],"metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:26:53.718627Z","iopub.execute_input":"2021-08-14T14:26:53.718934Z"},"trusted":true}},{"cell_type":"markdown","source":["## 16) Testing"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Get test data loss and accuracy\n","\n","test_losses = [] # track loss\n","num_correct = 0\n","\n","# init hidden state\n","test_h = model.init_hidden(batch_size)\n","\n","model.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    test_h = tuple([each.data for each in test_h])\n","\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    \n","    output, test_h = model(inputs, test_h)\n","    \n","    # calculate loss\n","    test_loss = criterion(output.squeeze(), labels.float())\n","    test_losses.append(test_loss.item())\n","    \n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","    correct = np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)\n","\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}\".format(test_acc))\n"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-07-23T09:51:23.957014Z","iopub.execute_input":"2021-07-23T09:51:23.957381Z","iopub.status.idle":"2021-07-23T09:51:25.085251Z","shell.execute_reply.started":"2021-07-23T09:51:23.95734Z","shell.execute_reply":"2021-07-23T09:51:25.084374Z"},"trusted":true}},{"cell_type":"markdown","source":["## Thank you "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}